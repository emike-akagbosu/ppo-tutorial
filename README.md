# ppo-tutorial
Using stablebaselines3 to implement PPO algorithm for RL tasks
https://github.com/DLR-RM/stable-baselines3.git

data -> contains csv files used to plot training results  
plot_results -> notebook for plotting results  
PPO_train_breakout.ipynb -> train model and create video to visualise performance  
PPO_train_mountain_car.ipynb -> train model and create video to visualise performance  
